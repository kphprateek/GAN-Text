{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bit0edfe5039cc346beab72df6273a634a8",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 俳句生成を実施する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "単語をidにしてGANに使えるようにする"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 観光地のレビューを575にした文字列を形態素解析して分かち書き\n",
    "\n",
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger('-Owakati -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd') #追加辞書を適用\n",
    "\n",
    "with open('/Users/sakasegawayosuke/Documents/programming/sotuken/data/hakataeki_haiku.txt') as f:\n",
    "    s = f.readlines()\n",
    "\n",
    "# print(s[0])\n",
    "result = \"\"\n",
    "for line in s:\n",
    "\n",
    "    # 前処理\n",
    "    line = line.replace('、', '')\n",
    "    line = line.replace('・', '')\n",
    "    line = line.replace('.', '')\n",
    "\n",
    "    # 分かち書き\n",
    "    temp = tagger.parse(line)\n",
    "\n",
    "    if  \")\" in temp or \"）\" in temp or \"（\" in temp or \"(\" in temp or \"「\" in temp or \"」\" in temp :\n",
    "        continue\n",
    "    else:\n",
    "        result += temp\n",
    "\n",
    "# print(result)\n",
    "\n",
    "# with open('/Users/sakasegawayosuke/Documents/programming/sotuken/data/reshape/hakataeki_haiku.txt', mode='w') as f:\n",
    "#     f.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 3216: 'サニーパン', 3217: 'による', 3218: '卒業', 3219: 'クリスマス', 3220: '余計', 3221: 'もい', 3222: 'すき', 3223: '最適', 3224: '9', 3225: '立ち', 3226: '待合い', 3227: '名品', 3228: 'クルーズ', 3229: 'なんだか', 3230: '軽食', 3231: '届き', 3232: '天候', 3233: 'りの', 3234: 'でる', 3235: '郵送', 3236: '日本全国', 3237: '知り', 3238: 'モスキート', 3239: 'わざわざ', 3240: '映え', 3241: 'わくわく', 3242: '取り', 3243: '物産展', 3244: '有人', 3245: '太宰府', 3246: '止まる', 3247: '目白押し', 3248: '１', 3249: '０度', 3250: '刺身', 3251: 'モール', 3252: '両方', 3253: '11月中旬', 3254: '幼稚園', 3255: '次', 3256: '部', 3257: '飽きる', 3258: 'ドラえもん', 3259: '急い', 3260: '別府温泉', 3261: '６年', 3262: '立つ', 3263: '見え', 3264: '覚え', 3265: 'アミューズメント', 3266: 'カクテル', 3267: 'へ', 3268: 'お昼', 3269: '鯛', 3270: '展望デッキ', 3271: '使い', 3272: '中心', 3273: '四川', 3274: '○', 3275: '動輪', 3276: '見つかり', 3277: '扉', 3278: 'お客さん', 3279: 'イロイロ', 3280: '急速', 3281: 'ミニオン', 3282: '激しく', 3283: '登場', 3284: '間隔', 3285: 'という', 3286: '出る', 3287: '家', 3288: '足元', 3289: '本数', 3290: '物色', 3291: '吉塚', 3292: '比べもの', 3293: '疲れ', 3294: '県', 3295: '事', 3296: '印象的', 3297: '４０分', 3298: '入り組ん', 3299: '号', 3300: '回', 3301: 'ぐんですこんなにいいえき', 3302: 'なんと', 3303: '華やか', 3304: '盛り上げ', 3305: '解り', 3306: '女性', 3307: '土曜', 3308: '笑笑', 3309: 'フォト', 3310: 'メキシコ', 3311: '牛', 3312: '買お', 3313: 'とも', 3314: 'とんこつラーメン', 3315: 'ラッシュ', 3316: '深夜', 3317: '賞味期限', 3318: 'リニューアル', 3319: '一番街', 3320: 'そのため', 3321: '広々', 3322: '始め', 3323: '商業地', 3324: '2番', 3325: '住民', 3326: '博多マルイ', 3327: 'ユニーク', 3328: 'のれ', 3329: '便利屋', 3330: '早', 3331: 'の', 3332: '食後', 3333: '目覚ましく', 3334: '数量限定', 3335: '不十分', 3336: '憩い', 3337: 'コロナ', 3338: '判り', 3339: '季節感', 3340: '経由地', 3341: '番線', 3342: '利用客', 3343: 'と共に', 3344: 'インフォメーション', 3345: '指定席', 3346: '用事', 3347: '運行', 3348: 'まわる', 3349: '2本', 3350: 'ぶらし', 3351: '反射', 3352: '思っ', 3353: 'ミルク', 3354: 'のるま', 3355: '着け', 3356: '東海', 3357: 'たたら侍', 3358: '住吉神社', 3359: '帰ら', 3360: '易く', 3361: '揃い', 3362: '路線バス', 3363: '末', 3364: '少し', 3365: 'あっち', 3366: 'チェーン店', 3367: 'くらぶ', 3368: '地点', 3369: '中島美嘉', 3370: 'とる', 3371: '地下駅', 3372: '秋', 3373: 'バタバタ', 3374: 'おいし', 3375: '当地', 3376: '旧', 3377: 'にかけて', 3378: '行ける', 3379: '観光地', 3380: '世界', 3381: '阪神', 3382: '細やか', 3383: 'ライオンキング', 3384: 'エビス', 3385: '56', 3386: '制限', 3387: '着き', 3388: '間', 3389: 'チープ', 3390: '買い物客', 3391: 'あっという間に', 3392: '必見', 3393: '厄介', 3394: 'ガイドブック', 3395: '駐車場', 3396: '鹿児島県', 3397: '360度', 3398: 'さつまいも', 3399: 'ワクワク', 3400: '杯', 3401: 'アミュハンズ', 3402: '為人', 3403: 'ロフト', 3404: '5月2日', 3405: 'ぶらつか', 3406: '建物', 3407: '食べに', 3408: 'ｍ', 3409: '目新しい', 3410: 'じゃない方', 3411: '無駄', 3412: '度', 3413: 'よっぽど', 3414: '展示', 3415: 'しょうが', 3416: '弱', 3417: 'シーズン', 3418: '旨い', 3419: '串', 3420: '始発列車', 3421: '記念', 3422: '連日', 3423: 'よけれ', 3424: '置い', 3425: '件', 3426: 'ワイン', 3427: '南口', 3428: '場所', 3429: '普段', 3430: 'アミュマルイ', 3431: '子供', 3432: '出迎え', 3433: '粥', 3434: '日帰り', 3435: '一つ', 3436: '覚悟', 3437: '食品売り場', 3438: 'ひよこ', 3439: 'ATM', 3440: '旅行者', 3441: '街並み', 3442: '３分', 3443: '表', 3444: 'クルーズトレイン', 3445: '層', 3446: 'withmallItisvery', 3447: '柄', 3448: '周', 3449: 'した', 3450: 'さっと', 3451: '管轄', 3452: 'いえ', 3453: '試食', 3454: '困り', 3455: '８０円', 3456: '西', 3457: 'くらい', 3458: '日本人', 3459: 'もちろん', 3460: '経験', 3461: '組ま', 3462: '乗り換える', 3463: 'あちこち', 3464: '12番', 3465: 'キャナルシティ', 3466: '済む', 3467: 'ヨドバシ', 3468: '仕上げる', 3469: 'られ', 3470: 'れる', 3471: '薄暗い', 3472: '名刹', 3473: '植え', 3474: 'しがみつい', 3475: '＆', 3476: '何度も', 3477: '商品', 3478: '早速', 3479: '流れる', 3480: '温泉', 3481: '何度', 3482: '丸', 3483: 'すごい', 3484: '立ち飲み', 3485: 'イス', 3486: '過ごす', 3487: '何しろ', 3488: 'なかなか', 3489: '来福', 3490: '都内', 3491: 'ち', 3492: '関西', 3493: '急用', 3494: '新八代', 3495: '新鮮味', 3496: 'ねー', 3497: '人混み', 3498: 'ミュージシャン', 3499: '主人', 3500: '舞台', 3501: 'Kitte', 3502: 'つっけんどん', 3503: 'manykindsoftrains', 3504: 'みあ', 3505: '喫茶店', 3506: '座席', 3507: '個々', 3508: 'すごく', 3509: '350円', 3510: '川内駅', 3511: '市街地', 3512: '｡', 3513: 'まだ', 3514: 'かいわい', 3515: '鍋', 3516: '食す', 3517: '手段', 3518: '相変わらず', 3519: '賄える', 3520: '概念', 3521: '金曜', 3522: '日々', 3523: 'どこ', 3524: 'アップ', 3525: 'アジア', 3526: 'さは', 3527: '中心的', 3528: '遣っ', 3529: '出入口', 3530: '装飾', 3531: '良かっ', 3532: '潰せ', 3533: '福岡市', 3534: 'ベテラン', 3535: '多少', 3536: '画像', 3537: '時間帯', 3538: '小物', 3539: '完璧', 3540: '好都合', 3541: 'トン', 3542: 'リース', 3543: 'アップダウン', 3544: '売っ', 3545: 'レストランスーパーコンビニパン', 3546: 'どっち', 3547: '帯', 3548: '汽車', 3549: 'かわっ', 3550: '効率', 3551: '有り', 3552: '三連休', 3553: '楽', 3554: 'だけ', 3555: '自転車', 3556: '食料品', 3557: '-', 3558: '風', 3559: '嫌い', 3560: 'いまだに', 3561: '瞬間', 3562: '燻', 3563: '大動脈', 3564: 'ギリギリ', 3565: '屋内', 3566: '名称', 3567: 'しょっちゅう', 3568: 'イルミネ', 3569: '出産', 3570: '表示', 3571: '滞在', 3572: 'メイン', 3573: '聴い', 3574: '丁寧', 3575: 'restaurantsaswell', 3576: '利用', 3577: '存在', 3578: '今や', 3579: 'っと', 3580: '3', 3581: '惹き', 3582: '青', 3583: '話題', 3584: '姿', 3585: '欠', 3586: 'KEＥＴ', 3587: '渡辺通', 3588: '何かと', 3589: 'コンサート', 3590: '洋食', 3591: 'デパート', 3592: '毎月', 3593: '堂', 3594: '有難い', 3595: '11月', 3596: '売店', 3597: '菓子類', 3598: '幅広い', 3599: '祭', 3600: 'つい', 3601: '見晴らし', 3602: '乗り継い', 3603: '素晴らしく', 3604: '五大都市', 3605: '人人', 3606: '特別展', 3607: 'すこし', 3608: '新しく', 3609: 'スタート', 3610: '増やし', 3611: '完全禁煙', 3612: '日付', 3613: '帰り際', 3614: '入場券', 3615: 'お話し', 3616: '博多ラーメン', 3617: '５０人', 3618: '兵衛', 3619: 'しか', 3620: '華々し', 3621: '防犯カメラ', 3622: '感動', 3623: '行内', 3624: 'でも', 3625: 'お祭', 3626: '贅沢', 3627: '九州', 3628: '館', 3629: 'ショピング', 3630: '電車', 3631: '探索', 3632: '板', 3633: '西側', 3634: '迷い', 3635: 'もし', 3636: 'のる', 3637: '音楽', 3638: 'そろい', 3639: '急', 3640: 'しまい', 3641: '一人', 3642: 'いくつか', 3643: '台湾', 3644: '子', 3645: '界隈', 3646: '一時間', 3647: '知っ', 3648: '野球', 3649: '大抵', 3650: 'せっかくだから', 3651: '会', 3652: '機会', 3653: 'キーホルダー', 3654: 'にがく', 3655: 'キティちゃん', 3656: 'しばらく', 3657: '数', 3658: '魅了', 3659: '近未来', 3660: '力', 3661: 'キャスター', 3662: '誘惑', 3663: '衣食住', 3664: 'あげ', 3665: '借りる', 3666: 'バイト', 3667: '名古屋駅', 3668: 'け', 3669: '発送', 3670: '00', 3671: '待ち', 3672: '衝撃', 3673: '1階', 3674: '♪♪', 3675: '３人', 3676: '失敗', 3677: '試し', 3678: '呑み', 3679: '付近', 3680: 'ひとりで', 3681: '通り過ぎ', 3682: 'ごちそう', 3683: '３回', 3684: '企画', 3685: '年代層', 3686: '歩か', 3687: '郵便ポスト', 3688: '経済的', 3689: 'デー', 3690: '法被', 3691: '売り場', 3692: '友達', 3693: '処', 3694: 'フラッと', 3695: '博多駅前', 3696: '素ラーメン', 3697: '櫓', 3698: 'いも', 3699: '入店', 3700: '困っ', 3701: 'スイスイ', 3702: '鍵', 3703: '朝定食', 3704: '少なかっ', 3705: '勘', 3706: '思える', 3707: 'どんたく', 3708: '静か', 3709: 'ますます', 3710: '発表', 3711: '一泊', 3712: 'ラッピング', 3713: '呼子', 3714: '友人', 3715: 'イルミネーション', 3716: 'も', 3717: '出て', 3718: '博多座', 3719: '会える', 3720: '選ん', 3721: '栄え', 3722: '進め', 3723: '睨み', 3724: '手間', 3725: '6番', 3726: '具体', 3727: 'あか', 3728: 'あいだ', 3729: '気持ち', 3730: '大きい', 3731: 'こだわり', 3732: '便', 3733: '乗ろ', 3734: 'ないっ', 3735: '相互', 3736: '乗り入れ', 3737: '人酔', 3738: 'やすく', 3739: 'マンマミーア', 3740: '想像', 3741: '見つかっ', 3742: 'レストラン', 3743: '客', 3744: '揃っ', 3745: '中心地', 3746: '半分', 3747: '分かる', 3748: 'ココ', 3749: '18', 3750: '軽く', 3751: 'よお', 3752: '押し', 3753: 'おいしかっ', 3754: '簡単', 3755: '賑わい', 3756: '人びと', 3757: 'allday', 3758: 'だら', 3759: 'たべ', 3760: 'エバンゲリオン', 3761: 'もらっ', 3762: '看板', 3763: '入っ', 3764: '５０万', 3765: '転勤', 3766: 'つくす', 3767: '3日', 3768: '盛りだくさん', 3769: '兎', 3770: '別れ', 3771: '世代', 3772: '故', 3773: 'いし', 3774: 'ヒレ', 3775: '遊ぶ', 3776: '長崎駅', 3777: '食べよ', 3778: '長居', 3779: '問題', 3780: '外来', 3781: '駅ビル', 3782: 'キヨスク', 3783: '証', 3784: 'プレミアム', 3785: 'ご', 3786: '格別', 3787: '二分', 3788: '20', 3789: 'ソーキそば', 3790: '歩く', 3791: 'じゃ', 3792: 'canfindeverything', 3793: '九州地方', 3794: '^_^', 3795: '凄い', 3796: '自動', 3797: 'ファション', 3798: '『', 3799: 'ビックリ', 3800: '締め', 3801: '覗い', 3802: 'たん', 3803: 'スタッフ', 3804: '離着陸', 3805: '営業', 3806: '整備', 3807: '客引き', 3808: '梅ケ枝', 3809: '定食', 3810: 'いえる', 3811: '5月4日', 3812: 'JR西', 3813: '細かく', 3814: '20年前', 3815: '決める', 3816: 'それで', 3817: '使い勝手', 3818: 'デッキ', 3819: '氏', 3820: 'あ', 3821: '後', 3822: 'おおい', 3823: 'たて', 3824: '広場', 3825: '飾っ', 3826: '区', 3827: '福岡空港駅', 3828: '名店街', 3829: '昼飲み', 3830: '柵', 3831: '本格的', 3832: 'ハウステンボス', 3833: 'やってくる', 3834: 'つばめ', 3835: '外国人', 3836: '活躍', 3837: '解決', 3838: '神社', 3839: '監修', 3840: '小さ', 3841: '遊び', 3842: 'バージョン', 3843: '２', 3844: 'たどり着き', 3845: 'かもめ', 3846: 'いち', 3847: '豚', 3848: '随分', 3849: '自然環境', 3850: '一', 3851: 'がよく', 3852: 'ウィンドウショッピング', 3853: '朝夕', 3854: '時計', 3855: 'チェック', 3856: '取り囲み', 3857: '専用', 3858: 'JR九州', 3859: '乗り過ごす', 3860: 'あからさま', 3861: '衛門', 3862: '若い', 3863: 'ステーション', 3864: '1人', 3865: '博多口', 3866: '日用品', 3867: '運転', 3868: 'マイングデイトスアミュプラザ', 3869: '金目鯛', 3870: 'まっちゃん', 3871: 'ただ', 3872: '奪わ', 3873: 'アミュプラザ', 3874: 'フリーパス', 3875: '広島', 3876: 'カステラ', 3877: '通行人', 3878: 'あんな', 3879: 'ミニオンズ', 3880: 'ゆえ', 3881: '酷い', 3882: '通年', 3883: 'シモンズベット', 3884: '少なめ', 3885: '始発駅', 3886: '東京駅', 3887: '進出', 3888: '満点', 3889: '揃え', 3890: 'ＫＩＴＴＥ', 3891: '弁当屋', 3892: '乗換え', 3893: 'むけ', 3894: '俺のフレンチ', 3895: '博多南駅', 3896: '出展', 3897: '敏感', 3898: 'やあ', 3899: '向き', 3900: 'ひろば', 3901: '閉店', 3902: '目立っ', 3903: '巨大', 3904: '値段', 3905: '味わえ', 3906: '同時に', 3907: 'シンプル', 3908: '座っ', 3909: '無し', 3910: '旅人', 3911: 'シティ', 3912: 'かわいい', 3913: '辛し', 3914: '活動', 3915: '個性', 3916: '考え', 3917: 'SL', 3918: 'ながの', 3919: 'とどまら', 3920: 'ありがたかっ', 3921: '買い物', 3922: '助かり', 3923: '他社', 3924: 'なっ', 3925: '帰れる', 3926: 'エヴァ', 3927: '結構', 3928: '昨年', 3929: 'です', 3930: '番号', 3931: '有意義', 3932: '掻き分け', 3933: '機能', 3934: 'ビール', 3935: '航空', 3936: '定番', 3937: '経過', 3938: '継続', 3939: 'そろう', 3940: '土日', 3941: '国鉄時代', 3942: '大きく', 3943: 'にくく', 3944: 'かえって', 3945: 'ファストフード', 3946: 'とっても', 3947: '訪れ', 3948: 'オフ', 3949: '手軽', 3950: 'エプロン', 3951: '準備', 3952: '一階', 3953: '選定', 3954: '過ぎ', 3955: 'だろ', 3956: '新しい', 3957: '搭乗', 3958: '殆ど', 3959: 'お供', 3960: '広がっ', 3961: 'ぱっと', 3962: '安全', 3963: 'うちに', 3964: 'ファッションビル', 3965: '降ろし', 3966: 'やもう', 3967: '基本的', 3968: 'とり', 3969: 'アパレル', 3970: '娘', 3971: 'ピザ', 3972: '500', 3973: '実感', 3974: '飲ん', 3975: '散歩', 3976: '泊まり', 3977: '動線', 3978: 'パパッと', 3979: 'Itisatransport', 3980: '午前', 3981: '目的地', 3982: 'ホットチョコレート', 3983: '去年', 3984: '店', 3985: 'aswell', 3986: 'もしくは', 3987: 'まる', 3988: 'どころ', 3989: '平日', 3990: '無', 3991: 'まさに', 3992: '先', 3993: 'ステキ', 3994: '度々', 3995: '紹介', 3996: '恥ずかしい', 3997: '葬式', 3998: 'おしい', 3999: 'ゴマサバ', 4000: 'かえ', 4001: 'つぶせる', 4002: 'プラス', 4003: '明確', 4004: '演奏', 4005: 'はじめて', 4006: '眺める', 4007: '中身', 4008: '特産', 4009: 'そう', 4010: 'おみやげ', 4011: '大都会', 4012: '寒く', 4013: '2回', 4014: '観る', 4015: '祇園山', 4016: '全店', 4017: 'はしご酒', 4018: '改札口', 4019: '女性関係', 4020: '頻度', 4021: '徒歩圏内', 4022: '6分', 4023: 'ラスク', 4024: 'ありか', 4025: '変化', 4026: '当たり前', 4027: 'セントラーザ博多', 4028: '撮れる', 4029: '観劇', 4030: '大', 4031: 'づくし', 4032: 'グイッと', 4033: '北側', 4034: '見回る', 4035: '極端', 4036: 'スケール', 4037: '短い', 4038: 'ずらし', 4039: '人ごみ', 4040: '良く', 4041: '煌', 4042: '近隣', 4043: '大規模', 4044: '12種', 4045: 'デザート', 4046: 'well', 4047: '陥没事故', 4048: '場合', 4049: '買える', 4050: 'とまる', 4051: '天神屋', 4052: 'ロケーション', 4053: '駐車券', 4054: '慣れる', 4055: '映っ', 4056: '喜ん', 4057: '冷静', 4058: '施さ', 4059: 'アミュブラザ', 4060: 'AMU', 4061: '着い', 4062: 'そ', 4063: '生まれ', 4064: 'レジャー', 4065: '見た目', 4066: '食べれ', 4067: '一際', 4068: '面', 4069: '使っ', 4070: '300円', 4071: 'ノー', 4072: 'チョコレート', 4073: 'ウインドショッピング', 4074: 'バージョンアップ', 4075: '見どころ', 4076: 'ぺー', 4077: '断然', 4078: '足ら', 4079: 'おもちゃのチャチャチャ', 4080: '照らさ', 4081: 'ずらす', 4082: '殺風景', 4083: '喫煙者', 4084: 'こっち', 4085: '迎え', 4086: 'うち', 4087: '共有', 4088: '銘打た', 4089: '省け', 4090: 'デ', 4091: 'グチ', 4092: 'さあ', 4093: 'すも', 4094: '見', 4095: 'やってき', 4096: '常に', 4097: '電飾', 4098: '撮っ', 4099: 'ゆふいん', 4100: 'オフィスビル', 4101: '東急ハンズ', 4102: '簾', 4103: 'きる', 4104: '横断歩道', 4105: 'にぎわい', 4106: '頂く', 4107: 'ともだち', 4108: '噴水', 4109: '施設', 4110: '往来', 4111: '選択肢', 4112: 'オフィス', 4113: '進む', 4114: '難しく', 4115: 'それなり', 4116: '向い', 4117: '脱帽', 4118: '学生さん', 4119: 'ゆふいんの森', 4120: 'デカ', 4121: '的', 4122: 'フリー', 4123: 'イメージ', 4124: '多め', 4125: 'バッチリ', 4126: '5年前', 4127: '宅配', 4128: 'あと', 4129: 'バスタミナール', 4130: 'TYPEEVA', 4131: '最近', 4132: 'わけ', 4133: '辛子明太子', 4134: 'バスターミタル', 4135: '遊び心', 4136: 'goodrestaurantsas', 4137: 'カップル', 4138: '迷う', 4139: 'オムライス', 4140: 'マトリョーシカ', 4141: '，', 4142: '多々', 4143: 'シネマ', 4144: 'それでも', 4145: 'やっ', 4146: 'ぶら', 4147: '下さい', 4148: 'ある時', 4149: 'ぐるぐる', 4150: '端', 4151: '歴史', 4152: '美味しく', 4153: '毎回', 4154: '翌日', 4155: 'できる', 4156: '区間', 4157: '買っ', 4158: '福', 4159: 'アミュキッテ', 4160: '出店', 4161: '路面店', 4162: 'しれ', 4163: '整っ', 4164: '仲間', 4165: '配色', 4166: 'ぜひ', 4167: '出会え', 4168: '差', 4169: '全国的', 4170: '映画館', 4171: 'あい', 4172: 'か所', 4173: '香っ', 4174: 'よー', 4175: '店屋', 4176: '方向', 4177: 'たくさん', 4178: '銀杏', 4179: 'まで', 4180: 'station', 4181: 'もの', 4182: '並べ', 4183: '通る', 4184: '格段', 4185: '集まる', 4186: 'また', 4187: '替える', 4188: '振っ', 4189: 'ひと', 4190: 'それら', 4191: 'insidethestation', 4192: '行き交い', 4193: '集約', 4194: '清潔感', 4195: 'ピンク色', 4196: 'スポット', 4197: 'アイデア', 4198: '15日', 4199: '到来', 4200: 'すもう', 4201: '靴', 4202: 'テレビ局', 4203: 'JR各線', 4204: 'りし', 4205: '使い物', 4206: '短く', 4207: 'さえ', 4208: '夏祭り', 4209: 'かわら', 4210: 'ほぼ', 4211: '地場', 4212: '入口', 4213: 'ショッピング', 4214: '今月', 4215: 'のみ', 4216: '悦', 4217: 'おつまみ', 4218: 'ミニトリップ', 4219: '入居', 4220: '回ら', 4221: 'すね', 4222: 'itis', 4223: 'すすめ', 4224: 'アーティスト', 4225: '嬉しく', 4226: '桜', 4227: '一大', 4228: '特有', 4229: '鉄道模型', 4230: 'いっ', 4231: '４０周年', 4232: '味わっ', 4233: '通路', 4234: '見渡せ', 4235: 'バー', 4236: 'フラメンコ', 4237: '駅弁', 4238: 'オムツ', 4239: '聞い', 4240: 'げ', 4241: '涼しく', 4242: '大きな', 4243: '星', 4244: '通常', 4245: '活気', 4246: '終了', 4247: '理由', 4248: 'なかろ', 4249: '目的', 4250: '一つひとつ', 4251: '飯店', 4252: '必ずや', 4253: '開い', 4254: '遊ば', 4255: '在来', 4256: '行き帰り', 4257: '酒蔵', 4258: '伝える', 4259: '伴い', 4260: '博多港', 4261: 'ちゃん', 4262: '役割', 4263: 'たびたび', 4264: 'つられ', 4265: '鍋屋', 4266: 'ワイワイ', 4267: '配置', 4268: '大会', 4269: '現れ', 4270: '比較', 4271: 'ちゃ', 4272: '見よ', 4273: 'こめ', 4274: '10年ぶり', 4275: '出勤', 4276: 'TNC', 4277: '関係', 4278: '鯖', 4279: '優しかっ', 4280: 'バス停', 4281: '立ち寄っ', 4282: '新大阪', 4283: '確実', 4284: '混乱', 4285: '道のり', 4286: '高', 4287: '湯布院', 4288: '多彩', 4289: '窓口', 4290: '指示', 4291: 'だ', 4292: 'に関する', 4293: '上げる', 4294: 'バル', 4295: '宅急便', 4296: '丼', 4297: 'オジャマ', 4298: '息づく', 4299: '\"', 4300: 'クロちゃん', 4301: 'お世話', 4302: '市', 4303: '逆', 4304: '大石', 4305: 'みやげ物', 4306: '親子', 4307: '戸惑い', 4308: '指定', 4309: '光っ', 4310: 'パート', 4311: '500系', 4312: '周年', 4313: 'いきかっ', 4314: '中洲', 4315: 'あたらしい', 4316: '居る', 4317: '最高', 4318: '！', 4319: '比べ', 4320: '効い', 4321: '【', 4322: '頼む', 4323: 'きとく', 4324: '乗り', 4325: '鳥栖駅', 4326: 'あんかけ', 4327: '設け', 4328: 'そんな', 4329: '念頭', 4330: 'ない', 4331: '費やす', 4332: '400円', 4333: '3人', 4334: '輝き', 4335: '動き', 4336: '貴重', 4337: '準備中', 4338: '地下中', 4339: '食堂', 4340: 'ドラック', 4341: '仕方', 4342: '改築', 4343: '見事', 4344: 'ごちそうさま', 4345: '山陽', 4346: '一望', 4347: '人出', 4348: 'デイトス', 4349: '喜び', 4350: 'もえり', 4351: '都', 4352: 'アピール', 4353: 'この', 4354: '泊', 4355: '閉口', 4356: '停める', 4357: '手荷物', 4358: ']', 4359: '乗車', 4360: '量', 4361: 'ところ', 4362: '素敵', 4363: '１０分', 4364: 'ハズレ', 4365: '発車', 4366: '7月1日', 4367: 'あり子', 4368: 'プチクロワッサン', 4369: 'ビジネスマン', 4370: '離陸', 4371: 'シテイ', 4372: 'つき', 4373: 'ましょ', 4374: 'ショッピングセンター', 4375: '基点', 4376: 'たより', 4377: '家族サービス', 4378: '何より', 4379: '平均', 4380: 'いる', 4381: 'びや', 4382: '福岡', 4383: 'きしめん', 4384: 'さすが', 4385: '大変', 4386: 'リッチ', 4387: 'たっく', 4388: 'こ', 4389: 'のぼせ', 4390: '10', 4391: '青春１８きっぷ', 4392: '是非', 4393: 'マルイ', 4394: '午前中', 4395: '疲れる', 4396: '無印良品', 4397: 'スケジュール', 4398: '終わり', 4399: '銀行', 4400: 'はか', 4401: '思わ', 4402: '満足感', 4403: '地下', 4404: '軍艦', 4405: '絶対', 4406: '料理', 4407: '輸入', 4408: '祭礼', 4409: '連ね', 4410: '昼', 4411: 'それに', 4412: '地方', 4413: '日中', 4414: '寄り', 4415: '第一', 4416: 'いわゆる', 4417: '普段使い', 4418: 'クレープ屋', 4419: 'stationalsoithas', 4420: '一風堂', 4421: '賞味', 4422: '付け', 4423: 'たい人', 4424: '種類', 4425: '集合', 4426: '後悔', 4427: 'さらに', 4428: 'ショッピングモール', 4429: '出き', 4430: '雑貨', 4431: '扱っ', 4432: 'まずは', 4433: '１５０万', 4434: '動い', 4435: '2019年', 4436: '応援', 4437: '月屋', 4438: '年代', 4439: '17時', 4440: '多かっ', 4441: '米', 4442: '空港駅', 4443: '目印', 4444: '30分', 4445: '呼ば', 4446: '貰える', 4447: '日月', 4448: 'はり', 4449: 'う', 4450: '伺い', 4451: '物', 4452: 'サラリーマン', 4453: '階', 4454: 'アワビ', 4455: 'かつ', 4456: '食い倒れ', 4457: 'もう少し', 4458: '求める', 4459: '含める', 4460: '正直', 4461: '廻れ', 4462: '時刻', 4463: '小倉駅', 4464: 'イタリアン', 4465: 'あきらめ', 4466: '大丸', 4467: 'コラボ', 4468: '満たし', 4469: '衣料', 4470: '昼飯', 4471: '道産子', 4472: '会場', 4473: '2つ', 4474: 'シーン', 4475: '連れ', 4476: '近く', 4477: '南北', 4478: '89階', 4479: '販売店', 4480: '終点', 4481: '月来', 4482: '品', 4483: 'スムース', 4484: 'ミニ', 4485: '郷土', 4486: 'コンパクト', 4487: '水戸岡鋭治', 4488: 'ハマっ', 4489: 'ガラリ', 4490: '寄れる', 4491: '見せ', 4492: '魅力的', 4493: '長い', 4494: 'かかっ', 4495: '恒例', 4496: 'カップ', 4497: '杜', 4498: 'パブリック', 4499: '10年', 4500: '店構え', 4501: '機関車', 4502: 'うろちょろ', 4503: '1２', 4504: 'ど', 4505: '来れ', 4506: '街', 4507: '厚', 4508: '発着', 4509: '交通手段', 4510: '余りに', 4511: '夕食', 4512: '気がつけ', 4513: 'き外', 4514: 'なんとか', 4515: '進ん', 4516: '床や', 4517: '夜遅く', 4518: '取り置き', 4519: '化し', 4520: 'ビッグ', 4521: 'おら', 4522: 'さと', 4523: 'フレンチ', 4524: 'あげく', 4525: '一気に', 4526: '石鹸', 4527: 'が', 4528: '続ける', 4529: '整然と', 4530: '思い', 4531: 'イチゴ', 4532: 'サイン', 4533: '停車駅', 4534: '大型', 4535: 'バッグ', 4536: '安く', 4537: 'とりあえず', 4538: '幅広く', 4539: 'しまっ', 4540: 'ディナー', 4541: '12月', 4542: '立ち止まる', 4543: '歓声', 4544: 'シート', 4545: 'レストランデパートスーパー', 4546: 'どの日', 4547: '在る', 4548: 'ゆき', 4549: '０円', 4550: '福岡空港', 4551: '嬉野', 4552: 'シチュエーション', 4553: '理解', 4554: '親戚', 4555: 'パスタ', 4556: '豊臣秀吉', 4557: '支え', 4558: '迷路', 4559: '長時間', 4560: '具', 4561: '好き', 4562: '抱え', 4563: '唐揚げ', 4564: 'スイーツ', 4565: 'ラッシュアワー', 4566: '様子', 4567: '500円', 4568: 'ラーメン屋', 4569: 'キャナルシティー', 4570: '土産物', 4571: 'ファッション', 4572: '日没', 4573: '美味い', 4574: '自販機', 4575: '待っ', 4576: '行き交う', 4577: '留まっ', 4578: '実家', 4579: '100', 4580: '広かっ', 4581: '販売', 4582: 'ジム', 4583: '昼ごはん', 4584: 'お客', 4585: 'バスタミーナル', 4586: '夕方', 4587: '例年', 4588: '飲み物', 4589: '土産', 4590: 'アミュ', 4591: '光', 4592: '通り抜け', 4593: 'LED', 4594: 'どんと', 4595: '料金', 4596: '回れる', 4597: '映し', 4598: '睨む', 4599: 'とれ', 4600: '使わ', 4601: '職場', 4602: '日持ち', 4603: '建ち', 4604: '56番', 4605: '鹿児島本線', 4606: '三輪車', 4607: '角打ち', 4608: '経', 4609: '待た', 4610: '無い', 4611: 'モダン', 4612: 'ゆふ', 4613: 'ちなみに', 4614: '入場', 4615: '骨', 4616: 'プレゼント', 4617: 'ストリート', 4618: '元', 4619: 'ねが', 4620: 'まくっ', 4621: '端末', 4622: '出かけ', 4623: 'リール', 4624: 'ことも', 4625: '通り越し', 4626: '連携', 4627: '呆気', 4628: '選べる', 4629: '支度', 4630: '食事処', 4631: '全県', 4632: 'たら', 4633: '達', 4634: '光の道', 4635: '控えめ', 4636: 'トレイン', 4637: '博多阪急', 4638: '暖房', 4639: '足せ', 4640: 'つくり', 4641: '案内', 4642: '定期的', 4643: '土曜日', 4644: 'KITEE', 4645: '有る', 4646: '酒', 4647: '遊べる', 4648: 'どんな', 4649: '祝', 4650: '慣れ', 4651: '出入り口', 4652: 'ちょっとした', 4653: 'めずらしい', 4654: '長蛇', 4655: 'はしご', 4656: 'かっ', 4657: 'かん', 4658: '終結', 4659: '長く', 4660: '東海道', 4661: 'いする', 4662: '週末', 4663: 'OIOI', 4664: '阪急', 4665: '当時', 4666: '女子', 4667: '目当て', 4668: 'マッチング', 4669: '客車', 4670: 'atKyushuNow', 4671: '煙', 4672: 'でどこ', 4673: '半端', 4674: '１０時', 4675: '単なる', 4676: 'おけ', 4677: 'アナウンス', 4678: '頼み', 4679: 'クオリティ', 4680: 'ひしめき合っ', 4681: '一歳', 4682: '泊まっ', 4683: 'ボディ', 4684: 'イートイン', 4685: '設計', 4686: 'itiscombined', 4687: '乗換', 4688: '思入れ', 4689: '調節', 4690: 'とんこつ', 4691: '子ども', 4692: '生まれ変わり', 4693: '5月', 4694: 'ママ友', 4695: '大幅', 4696: '合体', 4697: 'カフェ', 4698: 'お出かけ', 4699: 'もらい', 4700: '皆さん', 4701: '10歳', 4702: 'のに', 4703: '内装', 4704: '優れ', 4705: '１０年ぶり', 4706: 'オススメスポット', 4707: '予習', 4708: 'こまり', 4709: 'インプット', 4710: '欠かす', 4711: 'みどり', 4712: '昼前', 4713: '流石に', 4714: 'にくい', 4715: 'おやま', 4716: '高い', 4717: '言わ', 4718: '完結', 4719: '命名', 4720: 'た気', 4721: 'あるかな', 4722: '全て', 4723: 'あらゆる', 4724: '実', 4725: '有数', 4726: 'よい', 4727: '辛味', 4728: '居酒屋', 4729: '興味深', 4730: '好きな人', 4731: '８', 4732: '掲示', 4733: '重宝', 4734: '朝食', 4735: '比べる', 4736: 'KITTE', 4737: '＝', 4738: '乗り降り', 4739: '子連れ', 4740: '早々', 4741: '当たる', 4742: 'モツ', 4743: '肩', 4744: 'お隣', 4745: '立ち寄る', 4746: '員', 4747: '4', 4748: '堪能', 4749: '福岡駅', 4750: 'おいしく', 4751: '邪魔', 4752: '良さ', 4753: '人だかり', 4754: '途中', 4755: 'いじり', 4756: '座', 4757: '食器', 4758: 'コインロッカー', 4759: '自分', 4760: 'いただい', 4761: '安かっ', 4762: '鯨', 4763: '5%', 4764: 'ドラクエ', 4765: '一本', 4766: 'プレイルーム'}\n"
     ]
    }
   ],
   "source": [
    "# 分かち書きした単語にidをつけて辞書を作成\n",
    "\n",
    "with open('/Users/sakasegawayosuke/Documents/programming/sotuken/data/reshape/hakataeki_haiku.txt') as f:\n",
    "    s = f.readlines()\n",
    "\n",
    "temp_list = []\n",
    "\n",
    "for line in s:\n",
    "    line = line.split()\n",
    "    temp_list.extend(line)\n",
    "\n",
    "words = set(temp_list)\n",
    "# print(words)\n",
    "\n",
    "count = 1\n",
    "char_indices = {}  # 辞書初期化\n",
    "indices_char = {}  # 逆引き辞書初期化\n",
    "\n",
    "# 辞書の最初に文末記号を追加\n",
    "char_indices = {'EOS' : 0}\n",
    "\n",
    "for word in words:\n",
    "    if not word in char_indices:  # 未登録なら\n",
    "        char_indices[word] = count  # 登録する      \n",
    "        count +=1\n",
    "        # print(count,word)  # 登録した単語を表示\n",
    "# 逆引き辞書を辞書から作成する\n",
    "indices_char = dict([(value, key) for (key, value) in char_indices.items()])\n",
    "\n",
    "print(indices_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "804 3841 1857 2202 0\n4403 4664 4179 3331 1295 3176 3491 0\n3512 2519 1147 2666 1056 954 0\n1511 3331 1747 804 4462 3443 3197 0\n3627 3331 20 3984 4527 3763 0\n3984 4527 3763 1155 4380 3205 1683 4527 1612 0\n840 2394 19 1826 4291 856 4530 2761 0\n1433 2623 2654 555 4527 1156 2760 2206 0\n4382 4215 3079 249 3627 3331 786 0\n136 2394 4664 1730 3716 1971 1155 0\n3412 1857 794 1247 856 227 1247 1857 237 0\n2452 3197 1945 93 2357 4131 0\n2191 1857 2306 1155 4353 1826 2273 3396 0\n1569 1857 2104 1857 87 3197 1234 2344 4451 3197 0\n1294 3984 3716 3100 643 3205 2107 1857 0\n824 3554 2394 1717 4527 2931 2761 0\n1620 2388 3197 723 3295 4527 2601 1858 0\n275 2623 3428 2666 3488 4610 2787 3929 0\n2170 2394 3019 3929 4186 3040 1857 0\n1879 3331 1160 1857 2414 1858 2344 1826 0\n1836 136 2394 4748 1141 1858 0\n4139 4179 2089 3809 0\n2901 3364 3634 2761 4527 4722 786 0\n3576 2501 3331 3929 4527 1979 3716 1994 1835 2623 856 0\n136 2236 2666 1675 605 0\n3523 1857 1764 3331 3716 1192 3331 3009 856 2979 0\n3904 1857 2979 632 2472 1331 1857 0\n2073 4722 4527 136 3267 4658 0\n1910 1857 2607 856 1645 2623 2098 0\n1826 2394 1387 3197 1807 4255 0\n1160 2666 3198 2273 1764 2060 2666 1861 0\n243 1857 3716 59 2394 3080 1857 3716 0\n4641 4527 934 446 3205 780 0\n3779 1730 2394 3630 4527 1330 1155 3716 0\n4476 4527 1326 275 2394 2666 4330 1835 856 0\n4289 335 3197 363 1857 2404 0\n3515 2666 4465 2344 1298 1857 4237 3197 0\n2253 3716 4145 1155 2761 3205 3219 0\n610 3331 1401 2394 3921 3197 600 2761 0\n2871 2273 1051 2666 1456 0\n4508 2858 127 3331 3627 0\n3546 3716 2105 2344 3929 2724 4611 2394 0\n1241 3929 3627 3331 3272 0\n2119 2666 4382 2394 1326 3721 0\n3576 1095 1858 2344 3627 107 0\n2671 3197 311 1857 459 3331 2603 0\n3984 4527 643 3205 3257 1003 4527 4330 3929 0\n3715 4527 643 3331 3716 276 0\n1159 1348 2623 3331 3716 2206 2761 4527 0\n1003 4527 1141 1858 2344 4318 136 1857 372 93 0\n1718 1857 4687 0\n553 563 2344 648 1435 3331 0\n2796 1857 2404 4009 1857 2404 2761 644 0\n1401 3849 3197 2517 1857 1095 2344 0\n1209 4527 612 3554 2206 2467 709 0\n1683 4527 643 2718 2952 1155 1832 1294 0\n1839 2273 3331 1192 3331 3732 3716 290 0\n927 1857 2666 2203 856 1387 0\n412 3331 1294 804 786 2435 4527 3544 0\n1826 4179 3129 1858 2344 1839 2666 142 1632 1155 0\n4506 4527 2182 2635 2623 67 4527 2206 0\n1147 2666 1056 954 2394 1644 2344 2068 602 4353 0\n2707 1730 3331 2635 1857 1188 3331 3289 0\n1294 3693 3716 2386 1095 1155 1644 1155 0\n1722 2394 1143 3267 1672 1858 2344 3850 0\n260 1857 4324 1857 3129 1858 2344 2673 0\n2599 3331 639 4527 4380 235 1857 0\n1826 4664 3716 2206 1861 1257 0\n1387 3716 3328 1155 59 3716 3328 0\n1294 866 555 3197 33 4094 1155 696 0\n3591 1730 3135 3624 3361 2761 1826 0\n3873 2394 1971 3036 872 0\n2490 1861 2666 786 3921 804 2135 3197 0\n3858 3331 3630 2666 3915 0\n3806 1247 2108 1155 2081 79 2253 3716 0\n3425 3716 4655 4155 3331 3716 276 4291 856 0\n3799 1095 1155 1858 2344 3871 33 4094 1155 0\n2993 3716 1141 1858 2344 1095 786 3921 2501 0\n576 1857 4327 3469 1155 1644 1155 4380 3295 0\n2386 3716 3483 3929 4186 1294 0\n2787 4291 2623 856 4530 1858 2344 3627 3331 0\n1497 4291 50 856 4530 1858 2344 3652 0\n4009 3929 3624 1447 1213 1747 4527 3100 1155 0\n937 4527 4440 2344 3205 2513 2181 2344 0\n3576 1095 3738 2404 1858 2344 3781 3331 0\n1845 1644 1155 3716 696 2152 4330 1294 3716 0\n1826 2666 3627 3331 671 2394 0\n786 3984 4527 332 3391 1683 0\n412 804 4637 3716 3002 0\n1826 3929 3921 804 51 1730 3454 0\n3576 2601 2761 412 4589 866 555 804 0\n4589 2666 1839 2394 1337 2761 4318 4318 4186 2606 2394 0\n692 1095 1155 4056 2394 2438 2761 2251 1599 0\n2373 1857 2433 1364 1857 3951 1095 1155 0\n681 3197 2984 4330 1095 3135 3331 484 3716 1441 0\n1826 2273 1982 1578 1857 47 1155 0\n1192 1770 3716 2520 3929 4717 0\n3729 2957 3428 3929 2548 2090 2623 0\n3331 3929 4527 3051 3331 393 4527 643 0\n2278 1857 3104 3267 2748 1155 3630 3197 0\n786 3984 4527 2775 1141 1155 1644 1155 1982 0\n1294 3716 1839 2394 421 1155 3716 1994 2760 0\n275 3630 3716 3508 605 1025 2344 0\n722 2666 1612 1155 377 3576 1095 446 0\n1826 1857 1764 3652 4527 3100 1826 927 3331 0\n2320 1730 3331 4160 3716 1971 1155 0\n686 2666 3034 2386 1095 1155 0\n4208 1730 3697 4527 3461 2108 1155 2775 0\n2874 1837 2193 3716 3002 0\n3963 2985 3331 786 4406 3716 1605 1155 0\n786 3984 3197 2290 1155 4380 100 3929 2004 0\n866 555 2394 304 3929 1861 4301 1857 0\n4495 3331 3715 944 0\n1757 3267 1764 2068 3331 3166 871 0\n1294 3197 125 4361 3716 3135 2322 3716 0\n2606 4527 1117 3331 334 2394 0\n4630 4527 2206 377 275 3929 0\n4177 1971 1155 275 4291 856 4530 2761 0\n786 3984 4527 2601 1826 3331 3576 3159 2394 3716 0\n3921 3331 4123 2666 880 2344 3331 3929 0\n2421 2666 3158 446 3205 377 4561 0\n3290 2501 1857 2666 3223 711 305 0\n852 3149 856 2406 3149 3197 4475 1155 173 0\n605 2394 773 2623 1397 1972 2654 4527 0\n3984 4527 378 3197 4409 1155 1644 1155 136 0\n4131 2666 319 265 377 0\n2796 1857 2979 1465 4527 1939 0\n1819 3132 3066 1857 1357 0\n1770 4527 1095 3738 1155 3321 856 1095 1155 0\n3406 804 2088 1857 3364 2360 786 3984 0\n3911 1857 3318 1095 1155 877 0\n2144 1141 4330 3090 3956 3428 0\n3268 1331 2666 2122 2394 2761 4527 1991 0\n1826 2666 1253 610 4527 18 1095 1155 1644 0\n1387 804 3627 4255 0\n671 2394 453 2143 59 4527 2206 0\n2236 3331 4703 4527 377 605 2394 0\n3331 3197 3264 1155 1644 2761 4026 3929 4527 0\n3628 804 2183 3716 643 1984 0\n3331 2666 4382 2394 2666 136 3554 4291 856 0\n1120 3716 3100 3078 1155 4380 0\n3331 3929 4527 518 2666 4728 2394 3716 1199 0\n1747 4527 2121 3197 1498 1155 1644 1858 2344 0\n3104 927 1857 4079 0\n759 2815 1836 4527 755 2394 0\n380 3716 2206 2761 2631 4527 0\n3826 2394 2796 1857 2404 446 3929 786 3984 3716 0\n3995 1247 2108 1155 1644 2344 3205 3129 1858 2344 0\n2082 3331 1737 2666 377 162 2108 0\n4009 1025 2344 2221 4186 494 1764 1331 0\n697 3331 1294 4506 804 1984 0\n4196 4291 856 4530 2761 45 2273 0\n136 2666 3135 3624 3744 1402 2020 0\n2874 3716 3002 1247 2108 1155 2081 0\n2388 3331 2640 2666 498 1851 3929 0\n2720 2206 2837 4527 2386 1095 1155 0\n2387 4291 856 2775 3576 1095 1630 856 0\n3984 786 2468 4564 1719 2223 4181 0\n4630 3716 2386 1095 1155 2081 0\n3834 1013 1507 265 4719 3197 1095 0\n3075 1095 2344 1869 1857 4270 1095 1155 544 0\n1331 2666 3467 3331 3395 1857 0\n866 1857 1294 3691 856 686 1857 0\n2661 1247 2108 2344 1826 4318 3627 0\n888 3331 1147 4641 1324 1857 0\n20 3988 3331 786 3984 4527 1579 856 0\n809 29 3554 2394 1612 3429 3716 0\n4237 1025 2344 3546 3716 2105 2344 0\n3577 1095 2761 3205 4149 2916 1155 0\n136 4317 4318 4318 571 3624 3744 1155 0\n2357 4131 2666 4736 4527 0\n3984 804 1120 3716 2386 1095 0\n4213 804 2090 1730 3024 3331 3294 1857 0\n4213 2273 51 4179 4177 0\n3984 4527 2386 1095 1155 1644 2761 2060 2666 0\n477 4527 2206 3922 1858 2344 4318 500 0\n3703 804 2871 3433 4527 778 3470 0\n555 3716 1644 2344 974 80 1687 4527 2601 0\n3984 4527 2206 4213 3716 728 1003 0\n3774 3584 3120 1819 2666 2727 3716 1446 0\n2981 3716 3207 1247 2108 0\n3576 1095 1155 1644 2761 2857 2394 1294 0\n3080 2273 3716 1982 2273 3716 4476 0\n4589 866 555 3331 1597 3716 605 1857 3924 0\n671 3331 136 3929 3168 0\n4109 3716 2386 1095 1155 2761 2548 2091 2374 0\n870 1461 804 4396 0\n3865 2666 2253 3197 3100 4145 1155 0\n136 3331 1401 1857 2666 3374 4009 987 0\n3576 1095 1155 4380 3205 2072 3331 3428 0\n117 1025 2344 4697 804 3013 0\n192 2394 1430 2821 1857 3924 1155 1644 2344 462 0\n4564 1719 2223 4181 4527 4177 2206 0\n1826 862 2394 3987 3850 1845 2931 0\n4177 1376 1155 1644 2761 3257 3295 0\n4482 3889 4317 3929 3080 2394 3716 0\n975 2501 576 2666 1969 856 0\n3921 2394 1613 1764 3331 4527 3019 0\n3135 2394 3716 643 3205 3135 3412 1632 1155 3716 0\n1192 3331 630 1827 3554 2394 1612 0\n2635 2623 1826 2394 33 2623 224 3197 0\n1401 2666 3730 2631 0\n1387 856 4382 3331 1182 0\n1294 2654 3742 4722 4527 0\n2386 1095 1155 2761 4758 3197 0\n4181 646 3331 412 2394 778 3766 4702 0\n1132 1861 3865 2273 3576 0\n1826 3929 4637 3716 3763 1155 0\n2871 274 2725 1155 4380 3984 3716 643 0\n1229 453 2027 2515 0\n1979 3716 1994 1835 2623 856 4530 2761 1879 0\n256 3331 4752 2681 2199 3716 0\n1745 804 2301 2880 0\n4109 4527 3002 2501 3627 3331 0\n1845 1644 1155 3716 1820 4330 2635 2623 3428 3929 0\n3921 1857 275 930 1929 4220 1612 0\n136 862 3331 1271 4527 3721 0\n1819 3716 2995 4318 4318 206 2501 1250 0\n3857 3331 157 4527 1971 3036 1095 1155 250 0\n4444 3177 4179 1940 833 0\n4177 2206 2761 136 2236 0\n136 2666 3592 3331 2635 1857 3576 0\n3498 265 3197 3012 158 0\n1939 600 2761 4186 2234 1857 0\n1401 3267 3331 1779 1857 2666 2073 3716 59 0\n3657 3716 425 3100 3588 275 0\n1826 2666 4550 2273 59 0\n4433 3197 2582 1826 3716 698 1857 0\n3318 1095 1155 1644 1155 4213 1857 0\n136 2394 2666 1387 3331 1826 3716 0\n4571 804 51 1730 3716 2775 0\n671 3929 4527 1839 3197 3681 0\n922 1857 19 3929 136 3331 1401 0\n2722 2273 1764 3331 1857 2666 1839 3619 4330 0\n4453 1063 1826 862 1857 3716 523 0\n786 2435 4527 2206 2761 1387 1857 0\n1247 3197 722 249 1857 2153 1826 2666 3024 1857 4330 0\n3627 3331 2437 671 0\n365 4527 778 775 3331 2394 2666 856 3916 0\n4482 3197 964 2021 1187 0\n1247 4527 1054 2559 2344 3929 3841 3293 0\n3728 1857 3716 2174 2761 1095 4177 3331 1271 0\n59 3331 113 2394 3576 1095 1858 2344 0\n3962 3197 3027 1141 2761 2493 0\n136 4384 1857 3942 1155 698 0\n2428 804 3538 572 4179 3186 0\n3921 804 2135 1730 571 3624 2363 0\n1925 2394 3921 2666 1982 3331 0\n136 2666 412 3331 671 2394 0\n1171 3716 3763 1155 1644 1155 571 3624 2206 0\n4274 1697 1857 412 1857 34 1858 2344 4527 0\n1120 3716 2386 1095 1155 2081 0\n4181 1857 179 1003 4527 3508 1340 0\n20 2623 500 3984 3716 4177 0\n1028 2114 3331 2101 1401 3331 4237 0\n2170 3716 1579 1615 1155 4726 142 1632 0\n1819 804 2302 1730 20 3984 0\n3104 3554 3079 1663 4614 0\n4181 3619 880 3036 3935 4277 0\n4406 866 555 3331 3789 3331 2090 0\n1120 804 2654 4527 643 3205 0\n3129 72 3845 2394 2539 4683 0\n1936 3197 501 2344 1447 3561 2662 1857 0\n3909 4527 1971 2344 3205 3607 786 1096 1025 2344 0\n1839 3554 3624 3248 1845 1076 1155 2311 0\n1819 1294 3716 3100 3544 1155 2081 0\n3135 3624 3418 3929 4129 2666 0\n2192 3331 786 3984 4527 2206 2761 2719 0\n2818 3852 3716 0\n1563 3149 856 2406 3149 3197 4475 1155 173 3267 0\n412 3911 804 4664 3591 1857 0\n4196 1827 3716 62 1155 3017 3929 3135 0\n412 3331 4506 3197 2039 1147 1141 0\n1142 1857 3630 2394 2816 2761 1826 4403 0\n1747 319 3716 3100 3924 1155 159 0\n3715 2666 377 1241 1025 0\n4633 2666 2532 3219 1364 3331 2640 0\n786 4570 866 3554 2394 1612 4141 786 2435 0\n786 3984 3716 4385 2386 1095 1155 1644 1155 0\n4382 3331 1294 2666 2208 4271 1644 2761 0\n1747 4527 2886 2344 2273 3921 4527 1095 3738 0\n2309 2501 1003 4527 3100 58 3331 0\n1387 4527 643 856 419 1858 2344 4527 786 0\n1294 1857 4157 1155 25 2635 1857 1095 1155 1644 0\n1003 4527 4155 4213 2501 3428 0\n2435 3691 2666 377 2866 1155 1644 0\n907 1747 1857 3924 1155 2004 3929 0\n862 3554 2394 4213 3092 0\n110 3929 4527 1770 3716 2136 1857 0\n1324 4527 4177 2206 1858 2344 3781 0\n1003 4527 2601 2761 136 2236 1857 0\n4630 3781 3716 2206 275 2623 0\n1826 862 3331 1271 4527 3721 1402 2821 0\n275 3929 19 786 3984 4527 4177 0\n2617 1095 2369 645 1095 1858 0\n2776 3716 3763 1155 1644 0\n2104 1857 87 3197 1234 2344 4451 3197 558 0\n2060 2666 4430 4527 4561 2394 1603 3716 0\n4736 3716 2206 4385 825 4361 0\n961 2666 412 4527 1326 4291 0\n24 3716 2212 4177 3331 500 0\n1543 856 3775 1857 2666 4317 4361 3929 0\n3331 4527 1340 2472 2394 1683 4527 0\n2212 1025 2344 3715 3197 4094 1857 0\n3420 3197 3576 1095 1858 2344 1274 0\n275 2623 1826 3929 2548 1839 3197 3576 2501 0\n1368 63 4527 4177 4155 3428 0\n1227 2623 3205 1552 3197 2698 1095 1858 2344 4527 0\n3743 4527 3100 4094 3469 2761 1826 927 1857 0\n4109 3716 612 3205 377 2386 0\n1736 57 4330 3457 3331 4361 1857 0\n3019 3929 4174 3794 150 3331 2635 0\n3576 1095 2761 4242 1826 3929 4527 343 0\n2773 1857 2775 2206 2761 3109 0\n94 804 3921 4527 4155 1826 3929 0\n1770 4527 825 3929 3627 3331 786 0\n335 1260 3929 1294 866 555 1730 0\n2253 1331 2666 4427 1747 4527 3100 2404 0\n1003 2394 1987 1747 2394 859 1155 1644 2761 0\n3921 3716 136 2394 776 2004 0\n2614 4641 4527 3531 2344 3331 1835 2623 0\n1826 1827 3331 3933 2666 3459 3331 1003 0\n1826 2666 3578 1982 856 1816 3583 0\n1683 1857 2452 4527 1971 2344 1880 4527 1994 856 0\n2635 2623 3205 3921 2666 2373 4453 2394 0\n2073 2273 1279 1155 1826 1857 1764 3331 1857 2513 1857 0\n2806 3716 2809 1095 1155 2666 4281 1155 0\n3915 4527 1971 1155 1181 2394 2666 4330 2060 0\n136 4527 1826 3554 2394 1612 2436 0\n4358 2896 2302 2666 1242 880 0\n2253 1730 1649 4722 4527 1027 0\n2136 1857 1052 1857 2404 1858 1396 0\n1826 2273 3331 2453 3331 2386 0\n2423 2884 643 3331 2666 4317 3929 0\n4181 2666 612 3929 1095 2548 699 2119 3331 0\n3135 3624 2206 275 3929 1218 3331 2640 2666 0\n1181 4382 1857 2992 2501 1160 1857 2666 0\n412 3911 1821 1730 0\n866 1730 3716 2386 1095 1155 1644 1155 3576 1095 0\n20 2623 2246 4527 3763 0\n20 2623 4568 4527 3744 1155 4380 0\n4641 3197 558 1095 1155 2004 1192 0\n3401 2815 1836 4527 3082 0\n150 3331 3090 3929 1818 896 0\n4589 804 555 1730 3716 2775 643 3205 0\n1294 866 555 4527 2386 1095 1155 1644 1155 0\n4642 1857 4475 1155 2378 2761 402 0\n1747 3331 615 1247 1857 3075 1095 1858 2344 1159 0\n4531 2727 3197 200 2913 555 1857 3181 0\n94 1857 1632 1155 1644 2761 4018 0\n4739 2394 3716 600 2761 256 1857 0\n2504 4506 4350 0\n2258 3197 4239 1155 4380 856 2653 3579 1095 1155 159 2761 0\n786 3921 3197 2501 3331 1857 3716 377 275 0\n4382 3331 4289 1827 3331 3933 3331 0\n3080 4179 3716 59 2394 1869 2623 3205 0\n2134 3331 3435 3929 1612 1155 2666 3079 4330 0\n1836 3744 1155 4380 1617 1857 699 0\n1747 2666 4385 3929 2548 3523 1857 2771 0\n2858 1857 2816 2761 136 1857 2666 3921 0\n1683 833 3331 3388 1025 2344 4527 4213 0\n1770 3716 4040 1155 3591 3716 2087 0\n4722 3744 1155 1644 1155 2573 1508 3331 4629 2666 0\n2216 0\n576 3716 2206 2155 3716 2206 412 0\n1826 1857 4316 3331 1857 2167 2963 3331 487 0\n1778 2273 3331 72 3331 3566 0\n3092 866 555 4527 136 862 0\n275 4527 4040 1155 4382 3197 327 0\n3207 1095 1155 1644 1858 2344 1192 3159 0\n1879 2394 3841 1857 1632 2344 958 1857 786 0\n2896 2302 2666 1242 880 2344 0\n610 804 3591 4527 790 94 0\n4568 555 3197 2823 2501 3295 4527 0\n1845 2394 207 3331 1177 4291 0\n136 804 4260 2273 3716 4476 0\n1823 3019 1095 2761 3092 866 555 0\n1423 1857 643 3205 2275 2666 2073 2394 3716 0\n2721 1141 2761 2730 3248 1845 1644 1155 0\n623 4527 3508 1564 1095 0\n2323 2666 4482 3889 4527 840 2394 3024 0\n2293 1294 3716 2386 1095 1155 1644 0\n173 2621 4291 856 4530 2761 3080 0\n2435 3716 1382 2108 1155 4380 3205 2373 0\n3865 2892 3331 806 1857 3717 0\n1826 2623 3205 2871 2273 4385 1564 0\n2119 2666 3715 4527 377 0\n151 1860 2273 658 1247 2108 2904 0\n136 2666 1826 3554 3624 2522 648 0\n3248 4731 2906 3576 2394 260 1857 4324 0\n1860 2273 658 1247 2108 712 0\n3576 1095 3738 2979 3955 4449 2623 856 1050 0\n610 3331 1401 3331 3742 2394 94 804 0\n571 3624 2206 2761 2724 786 3984 4527 2775 0\n1294 866 555 3716 840 3929 1447 2773 2394 0\n2247 3197 3576 2501 1003 1857 3924 1155 1644 2344 0\n4317 2394 2282 4121 2623 2253 1730 0\n3351 1095 1155 2926 2787 3791 4330 1835 0\n991 1826 2666 4245 1857 859 1747 0\n1569 1857 3716 275 2251 94 3716 4177 0\n3715 4527 605 2394 4137 0\n1369 2273 4253 1402 500 0\n3412 1632 1155 3716 1820 1915 2787 3431 2273 0\n1387 2394 1632 2344 1160 1857 1279 0\n2765 1857 964 4196 1857 3924 1155 0\n3970 856 4157 1155 3759 1858 2344 2105 0\n2000 1857 2666 1869 4476 1857 896 4527 0\n1802 2206 1915 2787 1025 2344 4527 4131 2666 0\n2821 3197 4069 1155 2081 1858 2344 2472 0\n2056 4527 2477 1095 0\n4589 866 555 4527 1649 840 2623 3205 0\n3465 1857 1764 1569 1857 3576 1095 0\n1879 3331 2274 1857 2666 3223 3929 0\n3871 3209 1155 4650 4330 856 3634 2761 2548 0\n1387 4345 1387 856 0\n1247 3929 3219 3331 2253 1550 3716 0\n275 2623 856 2805 856 412 1857 34 1155 2273 0\n4017 2394 1340 1858 2344 2251 3209 0\n2526 571 2394 3716 643 1095 0\n1636 1857 600 2761 4318 4186 1294 0\n3380 2981 4527 3138 0\n469 856 580 1155 1644 2761 738 780 34 1155 3135 0\n2723 136 4318 3591 804 3742 0\n1986 2427 4137 1857 0\n3515 4527 432 3572 3331 1360 1857 0\n1736 3457 2394 2816 2761 1764 922 1857 0\n3984 804 1120 4527 2206 1717 0\n29 809 3331 1271 1181 3331 1271 0\n136 2666 4317 1857 19 3929 0\n4589 3716 4177 2206 1826 2394 4722 4527 0\n3318 1095 1155 2273 660 1857 2607 0\n94 2501 3428 4527 2386 1095 1155 0\n2796 1857 2404 1858 2344 4527 3086 555 4527 0\n224 4527 1794 2273 3263 2761 0\n3984 3716 3927 2206 1858 2344 1770 3716 0\n1683 1933 1857 3921 2647 2501 1857 0\n1747 3716 1513 3929 2883 3716 0\n3002 1095 1155 2081 2761 3205 786 3921 0\n3506 3197 1677 2501 3331 1857 2666 3799 0\n690 1444 1789 3331 0\n3027 1141 2761 2493 3197 3188 2344 0\n275 2623 3627 26 3331 1826 0\n380 3331 643 1826 3929 1294 804 555 3716 0\n116 4527 2886 2344 3205 856 829 3331 0\n3715 4527 558 1247 2108 1155 1644 1155 0\n2661 4527 3508 605 3929 4318 1401 0\n1770 3716 1095 3738 3034 275 3929 0\n3715 4527 605 2394 479 0\n239 3979 0\n3515 3331 786 3984 2666 1664 1079 1697 0\n2060 2666 4263 3921 1857 3129 2761 0\n3616 3197 1819 2850 2394 0\n4136 0\n1745 1730 2578 2394 2666 497 4330 2760 0\n1271 1926 479 3929 4318 2119 3824 0\n2661 1247 2108 1155 2081 377 1241 0\n282 2394 3970 856 4157 1155 3759 1858 0\n3200 173 823 856 2666 12 0\n1120 3716 4177 2206 3523 1857 0\n3628 3716 3763 1155 4380 3205 4417 0\n4280 4527 4696 1095 1155 1644 1155 3226 0\n1003 4527 4155 4336 2623 4196 3929 0\n1388 804 870 4527 643 3205 1683 3197 0\n2654 3716 4177 2206 2761 1294 0\n146 3331 487 4527 2293 1294 0\n2386 4318 3453 3197 1095 2369 3182 3716 0\n3984 2273 3389 2623 3984 4179 500 0\n136 2816 93 2004 4181 778 1832 0\n4589 192 2762 3197 311 1858 2344 3508 0\n3576 1095 446 1826 3929 728 4361 0\n3824 2394 2666 4662 1857 2507 3331 0\n3895 1857 1632 1155 2941 0\n1879 2394 2465 1970 1331 0\n786 3984 3331 798 3657 3716 857 3921 4527 0\n4475 3624 275 4291 856 4530 2761 1835 2404 0\n1757 1857 3716 972 2273 3576 1095 446 0\n1968 2623 3205 2039 4094 1155 1738 1857 0\n2873 2273 3331 947 2666 4317 0\n1294 3984 3716 2206 377 3721 1155 1644 0\n571 3624 3361 2761 3200 1857 643 0\n3428 3929 1835 2404 3209 1155 2578 2394 22 0\n4506 1857 1175 1095 2344 3921 94 0\n2397 1095 2761 4353 961 2666 0\n840 2394 412 51 3716 4748 1141 0\n3220 1857 1301 856 1095 2344 4123 0\n927 1857 2666 304 1120 643 0\n2871 2273 4385 1564 1095 1155 1644 1858 0\n4451 4527 571 3624 3361 2761 1095 2223 0\n4751 1095 1858 2344 3608 3781 4527 0\n3428 3606 4527 704 1155 3531 2344 0\n1826 862 1857 3716 4492 987 0\n3921 3716 1095 446 4318 1875 2513 3331 1845 1857 0\n4109 3002 2394 571 3624 3361 2761 0\n3281 2867 1155 1644 2344 3205 2871 4291 856 0\n33 2623 786 3984 4527 2206 2761 4527 498 0\n1294 3197 786 846 1025 4632 438 1857 0\n1387 2394 3576 4758 0\n1819 1730 33 2206 1263 1095 0\n1140 1095 1155 405 4632 958 1857 1869 2208 0\n4589 866 555 3716 2720 2206 275 3929 0\n1770 3716 2136 1857 4040 1155 565 0\n2236 2394 3716 786 2435 804 786 4589 3197 0\n786 4583 3523 2394 3777 4449 1835 3634 0\n1433 2394 1879 2816 93 2132 1857 412 0\n927 1857 2206 1545 2513 2181 2344 3331 0\n3576 1095 1858 2344 1564 1095 1155 1644 2344 3205 0\n412 2019 4527 1826 3624 964 0\n2051 1274 1302 4312 1025 2344 0\n1294 3197 4094 3036 2578 2955 1858 2344 0\n2333 2394 1982 856 4743 3197 3201 0\n1879 3331 958 2666 2480 1294 0\n1779 1857 1683 4527 1261 4330 3331 4527 1994 0\n866 555 4527 1073 1095 136 3554 0\n1826 4527 518 1429 2119 1857 1141 0\n1879 2394 412 3267 1932 1371 2344 3255 3331 0\n3428 4527 4547 1835 2887 1975 4330 1835 856 0\n4009 3929 407 3695 0\n3331 2394 2666 4330 95 4449 1835 1294 636 0\n4589 3984 4527 2770 4525 1294 0\n59 4527 3736 1747 4527 2136 1857 0\n276 3331 1716 3929 2548 1387 0\n1160 958 2666 2247 3197 3576 2501 1003 0\n275 2623 1826 3929 1294 3716 2775 0\n275 3929 1900 3168 3331 4382 0\n1241 1025 2344 3080 2273 3302 4230 0\n459 1857 2433 856 3032 4009 2394 2821 0\n798 3197 986 1155 4380 3205 3450 778 0\n304 2394 2727 4360 1857 565 1095 1155 0\n786 3984 923 0\n1241 2623 3205 1887 1857 3716 2404 0\n3984 3716 2386 1095 1155 2761 4318 142 1632 1155 0\n1003 4527 1141 2761 650 2394 1769 4242 0\n2631 3716 18 1095 1402 0\n1685 4179 1612 3921 3716 1095 104 2344 0\n4413 600 1155 565 4155 0\n1879 1141 2761 4428 0\n1821 1730 3331 3591 4527 0\n4481 4158 1095 3576 1095 2761 3080 0\n3269 4454 1730 1946 2501 3295 3197 0\n1794 1857 1282 4527 2206 237 0\n3984 4527 3505 3457 2394 3596 856 0\n275 3929 2776 804 0\n2939 1835 2897 3331 2635 2623 1228 4527 1095 1155 0\n1388 3716 2720 2206 2761 1192 3331 0\n450 3929 3026 1857 1777 2666 3576 0\n\n"
     ]
    }
   ],
   "source": [
    "# 辞書から文字列を単語のid列に変換する\n",
    "\n",
    "with open('/Users/sakasegawayosuke/Documents/programming/sotuken/data/reshape/hakataeki_haiku.txt') as f:\n",
    "    s = f.readlines()\n",
    "\n",
    "result = \"\"\n",
    "for line in s:\n",
    "    line = line.split()\n",
    "    # print(line)\n",
    "    # 単語に対応するidを並べて行く\n",
    "    for i in line:\n",
    "        # print(char_indices[i])\n",
    "        result += str(char_indices[i])\n",
    "        result += \" \"\n",
    "    result += \"0\" # 文末記号を追加\n",
    "    result += \"\\n\"\n",
    "\n",
    "print(result)\n",
    "\n",
    "with open('/Users/sakasegawayosuke/Documents/programming/sotuken/data/reshape/hakataeki_haiku2id.txt', mode='w') as f:\n",
    "    f.write(result)"
   ]
  },
  {
   "source": [
    "# dataloader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def dataset_for_generator(data_file, batch_size):\n",
    "    token_stream = []\n",
    "    with open(data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            line = line.split()\n",
    "            parse_line = [int(x) for x in line]\n",
    "            # if len(parse_line) == 20:\n",
    "            token_stream.append(parse_line)\n",
    "    return tf.data.Dataset.from_tensor_slices(token_stream).shuffle(len(token_stream)).batch(batch_size)\n",
    "\n",
    "def dataset_for_discriminator(positive_file, negative_file, batch_size):\n",
    "    examples = []\n",
    "    labels = []\n",
    "    with open(positive_file) as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            line = line.split()\n",
    "            parse_line = [int(x) for x in line]\n",
    "            if len(parse_line) == 20:\n",
    "                examples.append(parse_line)\n",
    "                labels.append([0, 1])\n",
    "    with open(negative_file) as fin:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            line = line.split()\n",
    "            parse_line = [int(x) for x in line]\n",
    "            if len(parse_line) == 20:\n",
    "                examples.append(parse_line)\n",
    "                labels.append([1, 0])\n",
    "    return tf.data.Dataset.from_tensor_slices((examples, labels)).shuffle(len(examples)).batch(batch_size)"
   ]
  },
  {
   "source": [
    "# rnnlm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Flatten\n",
    "import numpy as np\n",
    "\n",
    "class RNNLM(object):\n",
    "    def __init__(self, num_emb, batch_size, emb_dim, hidden_dim, sequence_length, start_token, learning_rate=0.01):\n",
    "        self.num_emb = num_emb\n",
    "        self.batch_size = batch_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_token = start_token\n",
    "        self.start_token_vec = tf.constant([start_token] * self.batch_size, dtype=tf.int32)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.grad_clip = 5.0\n",
    "\n",
    "        # LSTMのモデルを作成\n",
    "        # 1層のLSTM\n",
    "        self.g_model = tf.keras.models.Sequential([\n",
    "            Input((self.sequence_length,), dtype=tf.int32),\n",
    "            Embedding(self.num_emb, self.emb_dim, embeddings_initializer=tf.random_normal_initializer(stddev=0.1)),\n",
    "            LSTM(self.hidden_dim, kernel_initializer=tf.random_normal_initializer(stddev=0.1), recurrent_initializer=tf.random_normal_initializer(stddev=0.1), return_sequences=True),\n",
    "            Dense(self.num_emb, kernel_initializer=tf.random_normal_initializer(stddev=0.1), activation=\"softmax\")\n",
    "        ])\n",
    "        self.g_optimizer = self._create_optimizer(learning_rate, clipnorm=self.grad_clip)\n",
    "        if self.g_optimizer is not None:\n",
    "            self.g_model.compile(\n",
    "                optimizer=self.g_optimizer,\n",
    "                loss=\"sparse_categorical_crossentropy\")\n",
    "        else:\n",
    "            self.g_model.compile(\n",
    "                loss=\"sparse_categorical_crossentropy\")\n",
    "        self.g_embeddings = self.g_model.trainable_weights[0]\n",
    "\n",
    "    def target_loss(self, dataset):\n",
    "        # dataset: each element has [self.batch_size, self.sequence_length]\n",
    "        # outputs are 1 timestep ahead\n",
    "        ds = dataset.map(lambda x: (tf.pad(x[:, 0:-1], ([0, 0], [1, 0]), \"CONSTANT\", self.start_token), x))\n",
    "        loss = self.g_model.evaluate(ds, verbose=1)\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_batch(self):\n",
    "        # Initial states\n",
    "        h0 = c0 = tf.zeros([self.batch_size, self.hidden_dim])\n",
    "        gen_x = tf.TensorArray(dtype=tf.int32, size=self.sequence_length,\n",
    "                               dynamic_size=False, infer_shape=True)\n",
    "\n",
    "        def _g_recurrence(i, x_t, h_tm1, gen_x):\n",
    "            # o_t: batch x vocab, probability\n",
    "            # h_t: hidden_memory_tuple\n",
    "            o_t, h_t = self.g_model.layers[1].cell(x_t, h_tm1, training=False) # layers[1]: LSTM\n",
    "            o_t = self.g_model.layers[2](o_t) # layers[2]: Dense\n",
    "            log_prob = tf.math.log(o_t)\n",
    "            next_token = tf.cast(tf.reshape(tf.random.categorical(log_prob, 1), [self.batch_size]), tf.int32)\n",
    "            x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token)  # batch x emb_dim\n",
    "            gen_x = gen_x.write(i, next_token)  # indices, batch_size\n",
    "            return i + 1, x_tp1, h_t, gen_x\n",
    "\n",
    "        _, _, _, gen_x = tf.while_loop(\n",
    "            cond=lambda i, _1, _2, _3: i < self.sequence_length,\n",
    "            body=_g_recurrence,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
    "                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token_vec), [h0, c0], gen_x))\n",
    "\n",
    "        gen_x = gen_x.stack()  # seq_length x batch_size\n",
    "        outputs = tf.transpose(gen_x, perm=[1, 0])  # batch_size x seq_length\n",
    "        return outputs\n",
    "\n",
    "    def generate_samples(self, num_batches, output_file):\n",
    "        # Generate Samples\n",
    "        with open(output_file, 'w') as fout:\n",
    "            for _ in range(num_batches):\n",
    "                generated_samples = self.generate_one_batch().numpy()\n",
    "                for poem in generated_samples:\n",
    "                    print(' '.join([str(x) for x in poem]), file=fout)\n",
    "\n",
    "    def _create_optimizer(self, *args, **kwargs):\n",
    "        return None"
   ]
  },
  {
   "source": [
    "# Generator"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM\n",
    "import numpy as np\n",
    "# from rnnlm import RNNLM\n",
    "# from dataloader import dataset_for_generator\n",
    "\n",
    "class Generator(RNNLM):\n",
    "    def __init__(self, num_emb, batch_size, emb_dim, hidden_dim, sequence_length, start_token, learning_rate=0.01):\n",
    "        super(Generator, self).__init__(num_emb, batch_size, emb_dim, hidden_dim, sequence_length, start_token, learning_rate)\n",
    "        \n",
    "        # prepare model for GAN training\n",
    "        # GANのモデルを作成\n",
    "        self.g_model_temporal = tf.keras.models.Sequential(self.g_model.layers)\n",
    "        self.g_optimizer_temporal = self._create_optimizer(\n",
    "            learning_rate, clipnorm=self.grad_clip)\n",
    "        self.g_model_temporal.compile(\n",
    "            optimizer=self.g_optimizer_temporal,\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            sample_weight_mode=\"temporal\")\n",
    "\n",
    "# 事前学習用のモデル\n",
    "    def pretrain(self, dataset, target_lstm, num_epochs, num_steps, eval_file):\n",
    "        # dataset: each element has [self.batch_size, self.sequence_length]\n",
    "        # outputs are 1 timestep ahead\n",
    "        \n",
    "        # 途中経過を表示\n",
    "        def pretrain_callback(epoch, logs):\n",
    "            if epoch % 5 == 0:\n",
    "                self.generate_samples(num_steps, eval_file)\n",
    "                likelihood_dataset = dataset_for_generator(eval_file, self.batch_size)\n",
    "                test_loss = target_lstm.target_loss(likelihood_dataset)\n",
    "                print('pre-train epoch ', epoch, 'test_loss ', test_loss)\n",
    "                # buffer = 'epoch:\\t'+ str(epoch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n",
    "                # log.write(buffer)\n",
    "\n",
    "        ds = dataset.map(lambda x: (tf.pad(x[:, 0:-1], ([0, 0], [1, 0]), \"CONSTANT\", self.start_token), x)).repeat(num_epochs)\n",
    "        # 事前学習の実行\n",
    "        pretrain_loss = self.g_model.fit(ds, verbose=1, epochs=num_epochs, steps_per_epoch=num_steps,\n",
    "                                         callbacks=[tf.keras.callbacks.LambdaCallback(on_epoch_end=pretrain_callback)])\n",
    "        return pretrain_loss\n",
    "\n",
    " # GANの生成器のモデル\n",
    "    def train_step(self, x, rewards):\n",
    "        # x: [self.batch_size, self.sequence_length]\n",
    "        # rewards: [self.batch_size, self.sequence_length] (sample_weight)\n",
    "        # outputs are 1 timestep ahead\n",
    "        train_loss = self.g_model_temporal.train_on_batch(\n",
    "            np.pad(x[:, 0:-1], ([0, 0], [1, 0]), \"constant\", constant_values=self.start_token), x,\n",
    "            # sparse_categorical_crossentropy returns mean loss\n",
    "            # here we multiply (batch_size * sequence_length) to use weighted \"sum\"\n",
    "            sample_weight=rewards * self.batch_size * self.sequence_length)\n",
    "        return train_loss\n",
    "\n",
    "    def _create_optimizer(self, *args, **kwargs):\n",
    "        return tf.keras.optimizers.Adam(*args, **kwargs)\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.g_model.save_weights(filename, save_format=\"h5\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.g_model.load_weights(filename)\n"
   ]
  },
  {
   "source": [
    "# Target lstm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "# from rnnlm import RNNLM\n",
    "\n",
    "class TARGET_LSTM(RNNLM):\n",
    "    # function for reading save/target_params.pkl\n",
    "    # save/target_params.pklを読み込むための機能\n",
    "    def __init__(self, batch_size, sequence_length, start_token, params):\n",
    "        # Model sizes are determined by the parameter file\n",
    "        # モデルの大きさはパラメータファイルで決められる\n",
    "        num_emb = params[0].shape[0]\n",
    "        emb_dim = params[0].shape[1]\n",
    "        hidden_dim = params[1].shape[1]\n",
    "\n",
    "        super(TARGET_LSTM, self).__init__(num_emb, batch_size, emb_dim, hidden_dim, sequence_length, start_token)\n",
    "        weights = [\n",
    "            # Embedding\n",
    "            params[0],\n",
    "            # LSTM\n",
    "            np.c_[params[1], params[4], params[10], params[7]], # kernel (i, f, c, o)\n",
    "            np.c_[params[2], params[5], params[11], params[8]], # recurrent_kernel\n",
    "            np.r_[params[3], params[6], params[12], params[9]], # bias\n",
    "            # Dense\n",
    "            params[13],\n",
    "            params[14]\n",
    "        ]\n",
    "        self.g_model.set_weights(weights)"
   ]
  },
  {
   "source": [
    "# パラメータの設定"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#  生成器のパラメータ\n",
    "######################################################################################\n",
    "EMB_DIM = 32 # embedding dimension\n",
    "HIDDEN_DIM = 32 # hidden state dimension of lstm cell\n",
    "SEQ_LENGTH = 20 # sequence length\n",
    "START_TOKEN = 0\n",
    "PRE_EPOCH_NUM = 120 # supervise (maximum likelihood estimation) epochs\n",
    "SEED = 88\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#########################################################################################\n",
    "#  識別器のパラメータ\n",
    "#########################################################################################\n",
    "dis_embedding_dim = 64\n",
    "dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
    "dis_dropout_keep_prob = 0.75\n",
    "dis_l2_reg_lambda = 0.2\n",
    "dis_batch_size = 64\n",
    "\n",
    "#########################################################################################\n",
    "#  Basic Training Parameters\n",
    "# GANの学習を実行していく\n",
    "#########################################################################################\n",
    "\n",
    "TOTAL_BATCH = 3 # バッチサイズ\n",
    "\n",
    "# 学習で使用するデータ\n",
    "# 最初は存在しないので、lstmで作るらしい\n",
    "positive_file = 'save/real_data.txt'\n",
    "negative_file = 'save/generator_sample.txt'\n",
    "# このファイルはどうした？\n",
    "eval_file = 'save/eval_file.txt'\n",
    "\n",
    "# 生成された数\n",
    "generated_num = 10000"
   ]
  },
  {
   "source": [
    "# main関数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    assert START_TOKEN == 0\n",
    "\n",
    "    vocab_size = 5000\n",
    "\n",
    "    physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    if len(physical_devices) > 0:\n",
    "        for dev in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(dev, True)\n",
    "\n",
    "    # 生成器で学習\n",
    "    generator = Generator(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN)\n",
    "    # 最初のパラメータをpicklファイルから参照\n",
    "    target_params = pickle.load(open('save/target_params_py3.pkl', 'rb'))\n",
    "    target_lstm = TARGET_LSTM(BATCH_SIZE, SEQ_LENGTH, START_TOKEN, target_params) # The oracle model\n",
    "\n",
    "    # 識別器で学習\n",
    "    discriminator = Discriminator(sequence_length=SEQ_LENGTH, num_classes=2, vocab_size=vocab_size, embedding_size=dis_embedding_dim,\n",
    "                                  filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, dropout_keep_prob=dis_dropout_keep_prob,\n",
    "                                  l2_reg_lambda=dis_l2_reg_lambda)\n",
    "\n",
    "    # First, use the oracle model to provide the positive examples, which are sampled from the oracle data distribution\n",
    "    # GANの学習で使用する正解データを作成する\n",
    "    if not os.path.exists(positive_file):\n",
    "        target_lstm.generate_samples(generated_num // BATCH_SIZE, positive_file)\n",
    "    gen_dataset = dataset_for_generator(positive_file, BATCH_SIZE)\n",
    "    log = open('save/experiment-log.txt', 'w')\n",
    "\n",
    "\n",
    "    #  事前学習での文章生成をlstmで行い、生成器の重みを保存する\n",
    "    if not os.path.exists(\"generator_pretrained.h5\"):\n",
    "        print('Start pre-training...')\n",
    "        log.write('pre-training...\\n')\n",
    "        generator.pretrain(gen_dataset, target_lstm, PRE_EPOCH_NUM, generated_num // BATCH_SIZE, eval_file)\n",
    "        generator.save(\"generator_pretrained.h5\")\n",
    "    else:\n",
    "        generator.load(\"generator_pretrained.h5\")\n",
    "\n",
    "    # 識別器の事前学習での重み\n",
    "    if not os.path.exists(\"discriminator_pretrained.h5\"):\n",
    "        print('Start pre-training discriminator...')\n",
    "        # Train 3 epoch on the generated data and do this for 50 times\n",
    "        # 3エポックの識別器の訓練を５０回繰り返す\n",
    "        for _ in range(50):\n",
    "            print(\"Dataset\", _)\n",
    "\n",
    "            # まず生成器が偽物を作成\n",
    "            generator.generate_samples(generated_num // BATCH_SIZE, negative_file)\n",
    "\n",
    "            # 偽物と本物を混ぜたデータセットを作成\n",
    "            dis_dataset = dataset_for_discriminator(positive_file, negative_file, BATCH_SIZE)\n",
    "\n",
    "            # 識別器を学習させる\n",
    "            discriminator.train(dis_dataset, 3, (generated_num // BATCH_SIZE) * 2)\n",
    "        discriminator.save(\"discriminator_pretrained.h5\")\n",
    "    else:\n",
    "        discriminator.load(\"discriminator_pretrained.h5\")\n",
    "\n",
    "    rollout = ROLLOUT(generator, 0.8)\n",
    "\n",
    "    print('#########################################################################')\n",
    "    print('Start Adversarial Training...')\n",
    "    log.write('adversarial training...\\n')\n",
    "\n",
    "    # 学習の実行\n",
    "    # 今回は200回の訓練を行う\n",
    "    for total_batch in range(TOTAL_BATCH):\n",
    "        print(\"Generator\", total_batch)\n",
    "        # Train the generator for one step\n",
    "        for it in range(1):\n",
    "            samples = generator.generate_one_batch()\n",
    "            rewards = rollout.get_reward(samples, 16, discriminator)\n",
    "            generator.train_step(samples, rewards)\n",
    "\n",
    "        # Test\n",
    "        if total_batch % 5 == 0 or total_batch == TOTAL_BATCH - 1:\n",
    "            generator.generate_samples(generated_num // BATCH_SIZE, eval_file)\n",
    "            likelihood_dataset = dataset_for_generator(eval_file, BATCH_SIZE)\n",
    "            test_loss = target_lstm.target_loss(likelihood_dataset)\n",
    "            buffer = 'epoch:\\t' + str(total_batch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n",
    "            print('total_batch: ', total_batch, 'test_loss: ', test_loss)\n",
    "            log.write(buffer)\n",
    "\n",
    "        # Update roll-out parameters\n",
    "        rollout.update_params()\n",
    "\n",
    "        # Train the discriminator\n",
    "        print(\"Discriminator\", total_batch)\n",
    "        for _ in range(5):\n",
    "            generator.generate_samples(generated_num // BATCH_SIZE, negative_file)\n",
    "            dis_dataset = dataset_for_discriminator(positive_file, negative_file, BATCH_SIZE)\n",
    "            discriminator.train(dis_dataset, 3, (generated_num // BATCH_SIZE) * 2)\n",
    "    generator.save(\"generator.h5\")\n",
    "    discriminator.save(\"discriminator.h5\")\n",
    "\n",
    "    log.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ]
}